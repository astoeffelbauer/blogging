{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "portuguese-transformation",
   "metadata": {},
   "source": [
    "# Basic Reinforcement Learning using DeepMind's RL Framework \"Acme\"\n",
    "## Implement SARSA and Q Learning Agents in Acme\n",
    "### by Andreas Stöffelbauer\n",
    "\n",
    ">This notebook contains the code that accompanies my Medium blog post, written for Towards Data Science in June 2021.\n",
    "\n",
    "\n",
    "Acme is a research framework for reinforcement learning, open sourced by Google's DeepMind in 2020 . It was designed to simplify the development of novel RL agents and accelerate RL research. According to their own statement, Acme is used on a daily basis at DeepMind, who is spearheading research in reinforcement learning and artificial intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noted-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinforcement learning\n",
    "import acme\n",
    "from acme import types\n",
    "from acme.wrappers import gym_wrapper\n",
    "from acme.environment_loop import EnvironmentLoop\n",
    "from acme.utils.loggers import TerminalLogger, InMemoryLogger\n",
    "\n",
    "# environments\n",
    "import gym\n",
    "import dm_env\n",
    "\n",
    "# other\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-photography",
   "metadata": {},
   "source": [
    "## Blackjack Environment\n",
    "\n",
    "Acme agents are not designed to interact with Gym environments. Instead, DeepMind has their own RL environment API. You can think of the difference mainly in terms of how the timesteps are represented.\n",
    "\n",
    "Fortunately, however, you can still make use of Gym environments too since Acme's developers have provided wrapper functions for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "resistant-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Action Spec: Discrete(2)\n",
      "Reward Spec: (-inf, inf)\n"
     ]
    }
   ],
   "source": [
    "env = acme.wrappers.GymWrapper(gym.make('Blackjack-v0'))\n",
    "# env = acme.wrappers.SinglePrecisionWrapper(env)\n",
    "\n",
    "# print env specs\n",
    "env_specs = env.observation_space, env.action_space, env.reward_range # env.observation_spec()\n",
    "print('Observation Spec:', env.observation_space)\n",
    "print('Action Spec:', env.action_space)\n",
    "print('Reward Spec:', env.reward_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "apart-complaint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<StepType.FIRST: 0>, reward=None, discount=None, observation=(10, 9, False))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show a timestep\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-burke",
   "metadata": {},
   "source": [
    "As you can see, the timesteps are somewhat different to Open AI Gym environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-combat",
   "metadata": {},
   "source": [
    "## Actor, Learners, and Agents\n",
    "\n",
    "It is crucial to understand that there is a distinction between actors, learners, and agents. For the base class, see here: https://github.com/deepmind/acme/blob/master/acme/core.py. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-globe",
   "metadata": {},
   "source": [
    "### A Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fatty-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(acme.Actor):\n",
    "    \"\"\"A random agent for the Black Jack environment.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # init action values, will not be updated by random agent\n",
    "        self.Q = np.zeros((32,11,2,2))\n",
    "        \n",
    "        # specify the behavior policy\n",
    "        self.behavior_policy = lambda q_values: np.random.choice(2)\n",
    "        \n",
    "        # store timestep, action, next_timestep\n",
    "        self.timestep = None\n",
    "        self.action = None\n",
    "        self.next_timestep = None\n",
    "        \n",
    "    def select_action(self, observation):\n",
    "        \"Choose an action according to the behavior policy.\"\n",
    "        return self.behavior_policy(self.Q[observation])    \n",
    "\n",
    "    def observe_first(self, timestep):\n",
    "        \"Observe the first timestep.\" \n",
    "        self.timestep = timestep\n",
    "\n",
    "    def observe(self, action, next_timestep):\n",
    "        \"Observe the next timestep.\"\n",
    "        self.action = action\n",
    "        self.next_timestep = next_timestep\n",
    "        \n",
    "    def update(self, wait = False):\n",
    "        \"Update the policy.\"\n",
    "        # no updates occur here, it's just a random policy\n",
    "        self.timestep = self.next_timestep "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-monster",
   "metadata": {},
   "source": [
    "## Environment Loop\n",
    "\n",
    "If you already know a bit about reinforcement learning, and certainly if you have already implemented an RL algorithm, the following loop will look very familiar to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "polyphonic-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent()\n",
    "\n",
    "# make first observation\n",
    "timestep = env.reset()\n",
    "agent.observe_first(timestep)\n",
    "\n",
    "# run an episode\n",
    "while not timestep.last():\n",
    "    \n",
    "    # generate an action from the agent's policy and step the environment\n",
    "    action = agent.select_action(timestep.observation)\n",
    "    timestep = env.step(action)\n",
    "\n",
    "    # have the agent observe the timestep and let the agent update itself\n",
    "    agent.observe(action, next_timestep=timestep)\n",
    "    agent.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-exemption",
   "metadata": {},
   "source": [
    "Conveniently, there is a shortcut in Acme: the EnvironmentLoop, which performs pretty much exactly the steps seen above. You just have to pass your environment and agent instances and then you can run either a single episode or as many as you want with a single line of code. There are also various loggers available that track important metrics such as the number of steps taken in each episode and the collected rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "moderate-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use Acme training loop\n",
    "loop = EnvironmentLoop(env, agent, logger=InMemoryLogger())\n",
    "loop.run_episode()\n",
    "loop.run(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-british",
   "metadata": {},
   "source": [
    "## SARSA and Q Learning Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-spring",
   "metadata": {},
   "source": [
    "### Some Policies\n",
    "\n",
    "Some simple policies. Only `epsilon_greedy` is used by the agents in this notebook, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "resident-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform random policy\n",
    "def random_policy(q_values):\n",
    "    return np.random.choice(len(q_values))\n",
    "\n",
    "# greedy policy\n",
    "def greedy(q_values):\n",
    "    return np.argmax(q_values)\n",
    "\n",
    "# epsilon greedy policy\n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "    if epsilon < np.random.random():\n",
    "        return np.argmax(q_values)\n",
    "    else:\n",
    "        return np.random.choice(len(q_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-cutting",
   "metadata": {},
   "source": [
    "### SARSA Agent\n",
    "SARSA is an on-policy algorithm whose updates depend on the state, action, reward, next state, and next action (hence the name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "improving-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent(acme.Actor):\n",
    "    \n",
    "    def __init__(self, env_specs=None, epsilon=0.1, step_size=0.1):\n",
    "        \n",
    "        \n",
    "        # setting initial action values\n",
    "        self.Q = np.zeros((32,11,2,2))\n",
    "        \n",
    "        # epsilon for policy and step_size for TD learning\n",
    "        self.epsilon = epsilon\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        # set behavior policy\n",
    "        # self.policy = None\n",
    "        self.behavior = lambda q_values: epsilon_greedy(q_values, self.epsilon)\n",
    "        \n",
    "        # store timestep, action, next_timestep\n",
    "        self.timestep = None\n",
    "        self.action = None\n",
    "        self.next_timestep = None\n",
    "\n",
    "    def transform_state(self, state):\n",
    "        # this is specifally required for the blackjack environment\n",
    "        state = *map(int, state),\n",
    "        return state\n",
    "    \n",
    "    def select_action(self, observation):\n",
    "        state = self.transform_state(observation)\n",
    "        return self.behavior(self.Q[state])\n",
    "\n",
    "    def observe_first(self, timestep):\n",
    "        self.timestep = timestep\n",
    "\n",
    "    def observe(self, action, next_timestep):\n",
    "        self.action = action\n",
    "        self.next_timestep = next_timestep\n",
    "        \n",
    "    def update(self):\n",
    "        \n",
    "        # get variables for convenience\n",
    "        state = self.timestep.observation\n",
    "        _, reward, discount, next_state = self.next_timestep\n",
    "        action = self.action\n",
    "        \n",
    "        # turn states into indices\n",
    "        state = self.transform_state(state)\n",
    "        next_state = self.transform_state(next_state)\n",
    "        \n",
    "        # sample a next action\n",
    "        next_action = self.behavior(self.Q[next_state])\n",
    "\n",
    "        # compute and apply the TD error\n",
    "        td_error = reward + discount * self.Q[next_state][next_action] - self.Q[state][self.action]\n",
    "        self.Q[state][action] += self.step_size * td_error\n",
    "        \n",
    "        # finally, set timestep to next_timestep\n",
    "        self.timestep = self.next_timestep\n",
    "        \n",
    "sarsa = SarsaAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-receptor",
   "metadata": {},
   "source": [
    "To train SARSA on the environment for 500,000 episodes, simply run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "southwest-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = EnvironmentLoop(env, sarsa, logger=InMemoryLogger())\n",
    "loop.run(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "occupied-military",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_length</th>\n",
       "      <th>episode_return</th>\n",
       "      <th>steps_per_second</th>\n",
       "      <th>episodes</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.018209e+03</td>\n",
       "      <td>49996</td>\n",
       "      <td>77495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+200</td>\n",
       "      <td>49997</td>\n",
       "      <td>77496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000e+200</td>\n",
       "      <td>49998</td>\n",
       "      <td>77498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+200</td>\n",
       "      <td>49999</td>\n",
       "      <td>77499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+200</td>\n",
       "      <td>50000</td>\n",
       "      <td>77500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode_length episode_return  steps_per_second  episodes  steps\n",
       "49995               3            1.0      3.018209e+03     49996  77495\n",
       "49996               1            1.0     1.000000e+200     49997  77496\n",
       "49997               2            1.0     2.000000e+200     49998  77498\n",
       "49998               1            0.0     1.000000e+200     49999  77499\n",
       "49999               1            1.0     1.000000e+200     50000  77500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = loop._logger.to_dataframe()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-spelling",
   "metadata": {},
   "source": [
    "#### Q Learning Agent\n",
    "The Q learning agent below is very similar to the SARSA agent. They only differ in the way how updates to the Q matrix are made. This is because Q learning is an off-policy algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "underlying-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(acme.Actor):\n",
    "    \n",
    "    def __init__(self, env_specs=None, step_size=0.1):\n",
    "        \n",
    "        self.Q = np.zeros((32,11,2,2))\n",
    "        \n",
    "        # set step size\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        # set behavior policy\n",
    "        # self.policy = None\n",
    "        self.behavior_policy = lambda q_values: epsilon_greedy(q_values, epsilon=0.1)\n",
    "        \n",
    "        # store timestep, action, next_timestep\n",
    "        self.timestep = None\n",
    "        self.action = None\n",
    "        self.next_timestep = None\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        state = *map(int, state),\n",
    "        return state\n",
    "    \n",
    "    def transform_state(self, state):\n",
    "        # this is specifally required for the blackjack environment\n",
    "        state = *map(int, state),\n",
    "        return state\n",
    "    \n",
    "    def select_action(self, observation):\n",
    "        state = self.transform_state(observation)\n",
    "        return self.behavior_policy(self.Q[state])\n",
    "\n",
    "    def observe_first(self, timestep):\n",
    "        self.timestep = timestep\n",
    "\n",
    "    def observe(self, action, next_timestep):\n",
    "        self.action = action\n",
    "        self.next_timestep = next_timestep  \n",
    "\n",
    "    def update(self):\n",
    "        # get variables for convenience\n",
    "        state = self.timestep.observation\n",
    "        _, reward, discount, next_state = self.next_timestep\n",
    "        action = self.action\n",
    "        \n",
    "        # turn states into indices\n",
    "        state = self.transform_state(state)\n",
    "        next_state = self.transform_state(next_state)\n",
    "        \n",
    "        # Q-value update\n",
    "        td_error = reward + discount * np.max(self.Q[next_state]) - self.Q[state][action]        \n",
    "        self.Q[state][action] += self.step_size * td_error\n",
    "        \n",
    "        # finally, set timestep to next_timestep\n",
    "        self.timestep = self.next_timestep\n",
    "        \n",
    "qagent = QLearningAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "needed-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = EnvironmentLoop(env, qagent, logger=InMemoryLogger())\n",
    "loop.run(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fourth-gilbert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_length</th>\n",
       "      <th>episode_return</th>\n",
       "      <th>steps_per_second</th>\n",
       "      <th>episodes</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.003902e+03</td>\n",
       "      <td>9996</td>\n",
       "      <td>14970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000e+200</td>\n",
       "      <td>9997</td>\n",
       "      <td>14972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000e+200</td>\n",
       "      <td>9998</td>\n",
       "      <td>14974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000e+200</td>\n",
       "      <td>9999</td>\n",
       "      <td>14976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000e+200</td>\n",
       "      <td>10000</td>\n",
       "      <td>14978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_length episode_return  steps_per_second  episodes  steps\n",
       "9995               1           -1.0      1.003902e+03      9996  14970\n",
       "9996               2            1.0     2.000000e+200      9997  14972\n",
       "9997               2            1.0     2.000000e+200      9998  14974\n",
       "9998               2            1.0     2.000000e+200      9999  14976\n",
       "9999               2            0.0     2.000000e+200     10000  14978"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = loop._logger.to_dataframe()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-migration",
   "metadata": {},
   "source": [
    "Thanks for reading. Feel free to get in touch with me on [LinkedIn](https://www.linkedin.com/in/andreas-stoeffelbauer/) if you have any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-peoples",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvbandits",
   "language": "python",
   "name": "venvbandits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
